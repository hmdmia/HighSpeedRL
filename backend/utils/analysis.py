import numpy as np
import matplotlib.pyplot as plt
import pandas as pd

def get_reward_curve(agents):
    """
    Extract rewards from list of agents used in training
    :param agents: list of agents used in training
    :return: array of rewards
    """
    return np.array([agent.reward_total for agent in agents])


def choose_best(agents):
    """
    Find episode with higest reward value
    :param agents: list of agents used in training
    :return: agent with highest reward
    """
    rewards = get_reward_curve(agents)
    return agents[np.argmax(rewards)]


def moving_average(v, n):
    """
    Calculate a moving average (can be improved)
    :param v: vector of values
    :param n: number of samples across which to calculate average
    :return:
    """
    return np.convolve(v, np.ones(n)/n, 'valid'), np.array(range(n, len(v)+1))


def plot_average_reward_curve(agents, n=50):
    """
    Plot moving averge of a reward curve

    :param agents: list of agents corresponding to episodes
    :param n: number of samples across which to calculate average
    :return:
    """
    rewards = get_reward_curve(agents)
    mov_ave, episodes = moving_average(rewards, n=n)

    plt.figure()
    plt.plot(episodes, mov_ave)
    plt.xlabel('Episode')
    plt.ylabel('Average Reward for Last {:d} Episodes'.format(n))

    plt.show()


def run_network(initial_state, env, model):
    """
    Deterministically run a network after training to analyze performance

    :param initial_state: initial state of agent
    :param env: RL environment used for training
    :param model: RL model generated by training
    :return:
    """
    done = False
    obs = env.reset(initial_state=initial_state)
    while not done:
        action, _state = model.predict(obs, deterministic=True)
        obs, _, done, __ = env.step(action)

    env.agent.plot_state_history(style='segmented')


def run_network_for_shap(env, model, num_trials=100):
    """
    Run network after training to analyze with SHAP

    :param env: RL environment used for training
    :param model: RL model generated by training
    :param num_trials: number of trajectories to generate
    :return:
    """
    obs_list = []
    act_list = []
    done_list = []
    rew_list = []

    for trial in range(num_trials):
        done = False
        obs = env.reset()
        while not done:
            action, _state = model.predict(obs, deterministic=True)
            obs_list.append(obs)
            act_list.append(action)

            obs, rew, done, __ = env.step(action)
            rew_list.append(rew)
            done_list.append(done)

    return obs_list, act_list, rew_list, done_list


def run_network_stochastic(model, env, num_eps):
    """
    Stochastically run a network after training to analyze performance

    :param num_eps: number of episodes to use
    :param env: RL environment used for training
    :param model: RL model generated by training
    :return:
    """
    success_count = 0  # number of episodes within fpa tolerence and that reach target alt
    terminal_alt = []
    terminal_fpa = []
    terminal_time = []
    terminal_vel = []

    for x in range(0, num_eps):
        obs = env.reset()
        done = False
        while not done:
            action, _states = model.predict(obs, deterministic=False)
            obs, rewards, done, info = env.step(action)
        if done:
            terminal_time.append(obs[0])
            terminal_alt.append(obs[1])
            terminal_vel.append(obs[2])
            terminal_fpa.append(obs[3] * 180 / np.pi)
            if env.agent.success:
                success_count += 1

    success_percentage = success_count / num_eps
    print("success percentage ", success_percentage)

    # TODO make this into a function for post processing
    num_bins = 20
    counts_t, bins_t = np.histogram(terminal_time, bins=num_bins)
    counts_alt, bins_alt = np.histogram(terminal_alt, bins=num_bins)
    counts_vel, bins_vel = np.histogram(terminal_vel, bins=num_bins)
    counts_fpa, bins_fpa = np.histogram(terminal_fpa, bins=num_bins)
    fig, axs = plt.subplots(2, 2)
    axs[0, 0].hist(bins_t[:-1], bins_t, weights=counts_t)
    axs[0, 0].set_title('Final Time (s)')
    axs[0, 1].hist(bins_alt[:-1], bins_alt, weights=counts_alt)
    axs[0, 1].set_title('Terminal Alt (m)')
    axs[1, 0].hist(bins_vel[:-1], bins_vel, weights=counts_vel)
    axs[1, 0].set_title('Terminal Vel (m/s)')
    axs[1, 1].hist(bins_fpa[:-1], bins_fpa, weights=counts_fpa)
    axs[1, 1].set_title('Terminal FPA (deg)')
    fig.tight_layout()
    plt.show()


def run_network_save(initial_state, env, model, file = None, dir = None):
    """
    Deterministically run a network after training and save run data to npy file

    :param initial_state: initial state of agent
    :param env: RL environment used for training
    :param model: RL model generated by training
    :param file: filename for text file which contains trajectory, time, and control data
    :param dir: folder directory which contains saved run data
    :return:
    """
    done = False
    obs = env.reset(initial_state=initial_state)
    while not done:
        action, _state = model.predict(obs, deterministic=True)
        obs, _, done, __ = env.step(action)

    env.agent.save_run_data(file = file, save = initial_state, dir = dir)


def run_network_control(initial_state, env, model, save = None):
    """
    Deterministically run a network after training and plot control history and trajectory

    :param initial_state: initial state of agent
    :param env: RL environment used for training
    :param model: RL model generated by training
    :return:
    """

    done = False
    obs = env.reset(initial_state=initial_state)

    while not done:
        action, _state = model.predict(obs, deterministic=True)
        obs, _, done, __ = env.step(action)
    env.agent.plot_control(style='segmented', save =save)


def network_excel(initial_state, env, model, filename):
    """
    Save run data to an excel file

    :param initial_state: initial state of agent
    :param env: RL environment used for training
    :param model: RL model generated by training
    :param filename: name of the excel file to be saved
    :return:
    """

    obs0 = []
    obs1 = []
    obs2 = []
    obs3 = []
    rewards = []
    dones = []
    actions = []

    done = False
    obs = env.reset(initial_state=initial_state)

    reward = 0.
    obs0.append(obs[0])
    obs1.append(obs[1])
    obs2.append(obs[2])
    obs3.append(obs[3])
    dones.append(done)
    rewards.append(reward)
    while not done:
        action, _state = model.predict(obs, deterministic=True)
        obs, _, done, __ = env.step(action)
        actions.append(action)
        obs0.append(obs[0])
        obs1.append(obs[1])
        obs2.append(obs[2])
        obs3.append(obs[3])
        dones.append(done)
        rewards.append(reward)

    d = {'Time': obs0, 'Altitude': obs1, 'Velocity': obs2, 'FPA': obs3, 'reward': rewards, 'done': dones}
    df = pd.DataFrame(data=d)
    Xy = df[df['done'] == False]
    Xy = Xy[['Time', 'Altitude', 'Velocity', 'FPA']]
    Xy['Action'] = actions

    Xy.to_csv(filename+'.csv')


def run_network_success(initial_state, env, model):
    """
    Deterministically run a network after training to analyze performance

    :param initial_state: initial state of agent
    :param env: RL environment used for training
    :param model: RL model generated by training
    :return:
    """
    success = False
    ctr = 0
    done = False
    obs = env.reset(initial_state=initial_state)
    while not done:
        action, _state = model.predict(obs, deterministic=True)
        obs, _, done, __ = env.step(action)
        if done == True:
            if obs[0] <= 3000 & abs(obs[3]) * ((180 / np.pi) <= 0.25 * np.pi / 180):
                success = False
            else:
                success = True
    return success
